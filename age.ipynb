{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb5e93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "Female (25 ~ 32) surprise\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) sad\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) surprise\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (0 ~ 2) sad\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Male (25 ~ 32) angry\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Male (0 ~ 2) surprise\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (8 ~ 12) angry\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Male (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Male (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) angry\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) sad\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (0 ~ 2) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) happy\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) happy\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) happy\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) happy\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (0 ~ 2) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Male (4 ~ 6) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) surprise\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (8 ~ 12) happy\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Male (0 ~ 2) happy\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Male (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Male (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) sad\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (38 ~ 43) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (38 ~ 43) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) angry\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) angry\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Female (25 ~ 32) angry\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) angry\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) fear\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) fear\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) fear\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) fear\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) sad\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) angry\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Male (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Male (25 ~ 32) sad\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) happy\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) sad\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) surprise\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Male (8 ~ 12) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) sad\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) fear\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (25 ~ 32) angry\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) sad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (15 ~ 20) sad\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "Female (25 ~ 32) neutral\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "Female (15 ~ 20) neutral\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 140\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     cam \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./demo/dinner.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m--> 140\u001b[0m videoDetector(cam,cascade,age_net,gender_net,MODEL_MEAN_VALUES,age_list,gender_list, emotion_classifier, emotion_labels, emotion_target_size)\n\u001b[1;32m    142\u001b[0m cam\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    143\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mvideoDetector\u001b[0;34m(cam, cascade, age_net, gender_net, MODEL_MEAN_VALUES, age_list, gender_list, emotion_classifier, emotion_labels, emotion_target_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# cascade 얼굴 탐지 알고리즘\u001b[39;00m\n\u001b[1;32m     47\u001b[0m results \u001b[38;5;241m=\u001b[39m cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray1,\n\u001b[1;32m     48\u001b[0m                                    scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m,\n\u001b[1;32m     49\u001b[0m                                    minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     50\u001b[0m                                    minSize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     51\u001b[0m                                    )  \u001b[38;5;66;03m#for age, gender\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m faces \u001b[38;5;241m=\u001b[39m cascade\u001b[38;5;241m.\u001b[39mdetectMultiScale(gray2,\n\u001b[1;32m     53\u001b[0m                                  scaleFactor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m,\n\u001b[1;32m     54\u001b[0m                                  minNeighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     55\u001b[0m                                  minSize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m30\u001b[39m),\n\u001b[1;32m     56\u001b[0m                                  flags\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mCASCADE_SCALE_IMAGE\n\u001b[1;32m     57\u001b[0m                                  )\u001b[38;5;66;03m#for emotion\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     60\u001b[0m     x, y, w, h \u001b[38;5;241m=\u001b[39m box\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from statistics import mode\n",
    "from utils.datasets import get_labels\n",
    "from utils.inference import detect_faces\n",
    "from utils.inference import draw_text\n",
    "from utils.inference import draw_bounding_box\n",
    "from utils.inference import apply_offsets\n",
    "from utils.inference import load_detection_model\n",
    "from utils.preprocessor import preprocess_input\n",
    "\n",
    "USE_WEBCAM = True\n",
    "\n",
    "\n",
    "cv2.namedWindow('window_frame')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "frame_window = 10\n",
    "cam = None\n",
    "\n",
    "# 감정\n",
    "emotion_classifier = load_model('./models/emotion_model.hdf5')\n",
    "emotion_labels = get_labels('fer2013')\n",
    "emotion_target_size = emotion_classifier.input_shape[1:3]\n",
    "emotion_window = []\n",
    "\n",
    "\n",
    "def videoDetector(cam, cascade, age_net, gender_net, MODEL_MEAN_VALUES, age_list, gender_list,\n",
    "                  emotion_classifier, emotion_labels, emotion_target_size):\n",
    "\n",
    "    while True:\n",
    "        # 이미지 불러오기\n",
    "        ret, img = cam.read()\n",
    "\n",
    "        # 영상 압축\n",
    "        try:\n",
    "            img1 = cv2.resize(img, dsize=None, fx=1.0, fy=1.0)  #for age, gender\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        # 그레이 스케일 변환 \n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) #for age, gender\n",
    "        gray2 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #for emotion\n",
    "\n",
    "        # cascade 얼굴 탐지 알고리즘\n",
    "        results = cascade.detectMultiScale(gray1,\n",
    "                                           scaleFactor=1.1,\n",
    "                                           minNeighbors=5,\n",
    "                                           minSize=(20, 20)\n",
    "                                           )  #for age, gender\n",
    "        faces = cascade.detectMultiScale(gray2,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(30, 30),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                                         )#for emotion\n",
    "\n",
    "        for box in results:\n",
    "            x, y, w, h = box\n",
    "            face = img[int(y):int(y + h), int(x):int(x + h)].copy() #for age, gender\n",
    "            gray_face = gray2[y:y + h, x:x + w] #for emotion\n",
    "            blob = cv2.dnn.blobFromImage(face, 1, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n",
    "\n",
    "            try:\n",
    "                gray_face = cv2.resize(gray_face, (emotion_target_size)) \n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # emotion\n",
    "            gray_face = preprocess_input(gray_face, True)\n",
    "            gray_face = np.expand_dims(gray_face, 0)\n",
    "            gray_face = np.expand_dims(gray_face, -1)\n",
    "            emotion_prediction = emotion_classifier.predict(gray_face)\n",
    "            emotion_probability = np.max(emotion_prediction)\n",
    "            emotion_label_arg = np.argmax(emotion_prediction)\n",
    "            emotion_text = emotion_labels[emotion_label_arg]\n",
    "            emotion_window.append(emotion_text)\n",
    "\n",
    "            # gender\n",
    "            gender_net.setInput(blob)\n",
    "            gender_preds = gender_net.forward()\n",
    "            gender = gender_preds.argmax()\n",
    "            \n",
    "            # age\n",
    "            age_net.setInput(blob)\n",
    "            age_preds = age_net.forward()\n",
    "            age = age_preds.argmax()\n",
    "            \n",
    "            \n",
    "            # 분석 결과\n",
    "            info = f\"{gender_list[gender]} {age_list[age]} {emotion_text}\"\n",
    "            \n",
    "            # 화면 입력\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (255, 255, 255), thickness=2)\n",
    "            cv2.putText(img, info, (x, y - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "            \n",
    "            # 콘솔 출력\n",
    "            print(info)\n",
    "\n",
    "        # 영상 출력\n",
    "        cv2.imshow('facenet', img)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):  \n",
    "            break\n",
    "\n",
    "# 얼굴 탐지 모델 가중치\n",
    "cascade_filename = './models/haarcascade_frontalface_alt.xml'\n",
    "# 모델 불러오기\n",
    "cascade = cv2.CascadeClassifier(cascade_filename)\n",
    "\n",
    "\n",
    "MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n",
    "\n",
    "# 나이\n",
    "age_net = cv2.dnn.readNetFromCaffe(\n",
    "\t'./models/deploy_age.prototxt',\n",
    "\t'./models/age_net.caffemodel')\n",
    "\n",
    "# 성별\n",
    "gender_net = cv2.dnn.readNetFromCaffe(\n",
    "\t'./models/deploy_gender.prototxt',\n",
    "\t'./models/gender_net.caffemodel')\n",
    "\n",
    "\n",
    "age_list = ['(0 ~ 2)','(4 ~ 6)','(8 ~ 12)','(15 ~ 20)',\n",
    "            '(25 ~ 32)','(38 ~ 43)','(48 ~ 53)','(60 ~ 100)']\n",
    "gender_list = ['Male', 'Female']\n",
    "\n",
    "\n",
    "\n",
    "if (USE_WEBCAM == True):\n",
    "     cam = cv2.VideoCapture(0)  # Webcam source\n",
    "\n",
    "else:\n",
    "    cam = cv2.VideoCapture('./demo/dinner.mp4') \n",
    "\n",
    "\n",
    "\n",
    "videoDetector(cam,cascade,age_net,gender_net,MODEL_MEAN_VALUES,age_list,gender_list, emotion_classifier, emotion_labels, emotion_target_size)\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54cf52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
